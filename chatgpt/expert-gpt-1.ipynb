{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anubhavtomar/2023_projects/chatgpt/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# write a prompt to define the input and output of a gpt call - expert gpt in a specific task\n",
    "from utils import get_openai_client, read_web_content, query_system_expert ,extract_message_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_openai_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bse_markets = 'https://www.bseindia.com/markets.html' #crap\n",
    "# reddit_thread = 'https://www.reddit.com/r/MachineLearning/comments/1bho0r0/in_2024_which_library_is_best_for_time_series/' #blocked\n",
    "medium_article = 'https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37'\n",
    "members_medium_article = 'https://medium.com/@pspddo/new-python-tools-that-deserve-your-attention-02df2fb987a8'\n",
    "# content = read_web_content(medium_article)\n",
    "\n",
    "# content.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets write an agent that can parse the response from the jina reader api. It is builtl for medium type articles.ll\n",
    "# ideally the medium article should return well structured markdown from reader api -> infer that structure -> return as json \n",
    "# classify the story as members or free based on the content recvd -> if its not full article then classify as members only and its a failure for story collecting agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_response_type = {\n",
    "    \"meta\": {\n",
    "        \"status\": \"success\" ,\n",
    "         \"message\": \"if error then ....\" # Or \"error\" if there's an issue\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"article_insights\": {\n",
    "        \"title\": \"...\",\n",
    "        \"url\": \"...\",\n",
    "        \"publislhed_time\": \"...\",\n",
    "        \"key_topics\": [\"...\", \"...\", \"...\"],\n",
    "        \"summary_200\" : \"summary in 200 words\",\n",
    "\n",
    "        \n",
    "        \"code_snippets\": [\n",
    "            {\n",
    "            \"language\": \"...\",\n",
    "            \"code\": \"...\"\n",
    "            },\n",
    "        ]\n",
    "        },\n",
    "        \"links\": ['list','of','links in the article' ] \n",
    "    \n",
    "        # \"cleaned_article\": \" ... cleanup any special characters or spaces in the article and put under the key. keep full article no summarising\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze the provided text and extract the available information in the form of structure provided at the end.  \\n                    FIRST AND  MOST IMPORTANTLY: if the content provided isnt long enough or COMPLETE, or DOESNT LOOK  like a article to you. Update status as failed and message as \"MEMBERS_ONLY\".\\n                    meta.status as warning. if the content is too big. (more than 2000 words).\\n                    Only parse the unstructure data according to the above structure. \\n                    if most of the content is not fiiting in the structure of data, update the meta.status to failed, with additional messagelike \"NO ARTICLE FOUND\".\\n                    Remember that you are an expert in parsing unstructured markdown content into json, recognising entities very easily.\\n                    Response structure : {\\'meta\\': {\\'status\\': \\'success\\', \\'message\\': \\'if error then ....\\'}, \\'data\\': {\\'article_insights\\': {\\'title\\': \\'...\\', \\'url\\': \\'...\\', \\'published_time\\': \\'...\\', \\'key_topics\\': [\\'...\\', \\'...\\', \\'...\\'], \\'summary_200\\': \\'summary in 200 words\\', \\'code_snippets\\': [{\\'language\\': \\'...\\', \\'code\\': \\'...\\'}]}, \\'links\\': [\\'list\\', \\'of\\', \\'links in the article\\']}}.\\n\\n                        '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt_full = f\"\"\"Analyze the provided text and extract the available information in the form of structure provided at the end.  \n",
    "                    FIRST AND  MOST IMPORTANTLY: if the content provided isnt long enough or COMPLETE, or DOESNT LOOK  like a article to you. Update status as failed and message as \"MEMBERS_ONLY\".\n",
    "                    meta.status as warning. if the content is too big. (more than 2000 words).\n",
    "                    Only parse the unstructure data according to the above structure. \n",
    "                    if most of the content is not fiiting in the structure of data, update the meta.status to failed, with additional messagelike \"NO ARTICLE FOUND\".\n",
    "                    Remember that you are an expert in parsing unstructured markdown content into json, recognising entities very easily.\n",
    "                    Response structure : {system_response_type.__str__()}.\n",
    "\n",
    "                        \"\"\"\n",
    "system_prompt_chunk = f\"\"\"Analyze the provided text and decide wether the article is complete or is cut down? \n",
    "                    if the article is members only it would only have some starting part of the actual article, you need to identify it and return\n",
    "                    FULL_ARTICLE, INCOMPLETE_ARTICLE, UNIDENTIFIED_CONTENT based on the following:\n",
    "                    UNIDENTIFIED_CONTENT:  when the input content does not look like a BLOG OR AN ARTICLE. but some random website homepage or naviagtions content then mark it as UNIDENTIFIED_CONTENT.\n",
    "\n",
    "                    Remember that you are an avid reader and recognise how humans read and write documentation and stories. The input is passed by a scraper that has already removed most html and gives you markdown. So use that.\n",
    "                    FULL_ARTICLE: Only say FULL_ARTICLE when you are 200% sure that the content provided is a BLOG OR ARTICLE like news or educational written by people.\n",
    "                    INCOMPLETE_ARTICLE: this is a rare case when the provided link is under pay wall. so recognise these paywalls and ONLY WHEN you are sure its incomplete due to the pay-wall, mark as INCOMPLETE.\n",
    "                    \"\"\"\n",
    "system_prompt_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read a chosen article - success case\n",
    "reader = read_web_content(medium_article)\n",
    "content = reader.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_content': b'Title: How to perform anomaly detection in time series data with python? Methods, Code, Example!\\n\\nURL Source: https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37\\n\\nPublished Time: 2023-06-30T04:11:11.678Z\\n\\nMarkdown Content:\\n[![Image 1: Yu Rong (Tammy) Tsao](https://miro.medium.com/v2/resize:fill:88:88/1*LU6YltLV-yHZXRsTTv9fng@2x.jpeg)](https://medium.com/@goldengoat?source=post_page-----e83b9c951a37--------------------------------)\\n\\n![Image 2](https://miro.medium.com/v2/resize:fit:700/1*CCLlnUVtuNBo7idKtODghQ.png)\\n\\nIn this article, we will cover the following topics:\\n\\n*   Why our business needs anomaly detection?\\n*   Types of anomaly and how to choose the anomaly detection algorithms\\n*   The brief concept of level shift anomaly detection and isolation forest method\\n*   The implementation details in python\\n*   A real-world example. Includes the encountered problem of the business scenario and the result\\n\\nSo that\\xe2\\x80\\x99s start!\\n\\nWhy our business needs anomaly detection?\\n-----------------------------------------\\n\\nOne of the daily routines of a data analyst is to inspect tons of metrics in dashboards and check if there is any unusual user behavior or drops in business performance. The main challenge is when we see a lower (or higher) value, how do we determine whether it is a normal data fluctuation or a genuine anomaly that we should dive into the root cause to prevent revenue loss? Furthermore, it takes lots of time if we inspect all metrics manually. Also, we may skip some less commonly used metrics, which will probably lead to delayed alerts if some unexpected abnormal events happened. Therefore, we should develop an anomaly detection system to identify data points that deviate significantly from the general behavior of the data and provide early warning of unusual patterns, which can help us stay ahead of potential threats or problems.\\n\\nDecide the key metrics and Identify data types\\n----------------------------------------------\\n\\nIn most business scenarios involving app applications, the primary metrics that need to be monitored are univariate time series data. These metrics include DAU, retention rate, conversion rate, etc. Additionally, these data points are often unlabeled, making it challenging to use supervised learning methods for anomaly classification. As a result, it is more practical to employ unsupervised methods that rely solely on time series data and domain knowledge.\\n\\nTypes of time series anomalies\\n------------------------------\\n\\nGenerally, there are three main types of anomaly, which are (1) point anomaly, (2) contextual anomaly and (3) collective anomaly. Many articles have [introduced the difference among these types](https://rapidminer.com/glossary/anomaly-detection/) and how to implement the relative anomaly detection algorithm. On the other hand, [ADTK](https://adtk.readthedocs.io/en/stable/index.html) (Anomaly Detection Toolkit) also introduced common anomaly types of time series data. Such as spike, level shift , pattern change, and seasonality, etc. The [document](https://adtk.readthedocs.io/en/stable/userguide.html) also recommended the appropriate detector for each anomaly type.\\n\\nIn our case, we mainly focus on two types of anomalies: `Level Shift Anomalies` (Fig 1) and `Collective Anomalies` (Fig 2). Level Shift Anomalies help us detect sudden drops or increases in data values, enabling us to provide timely alerts. On the other hand, Collective Anomalies are a collection of similar data points that can be considered abnormal together when compared to the rest of the data. This type of anomaly can allow us to continuously monitor data points and issue warnings when they have not returned to normal.\\n\\n![Image 3](https://miro.medium.com/v2/resize:fit:603/1*YbMZ6Fy4hD8085fhnvekeg.png)\\n\\nFig 1. Level shift anomaly. | Image by [ADTK user guide](https://arundo-adtk.readthedocs-hosted.com/en/stable/userguide.html).\\n\\n![Image 4](https://miro.medium.com/v2/resize:fit:700/1*L5ZOAdKgomhZynD0x1_PcA.png)\\n\\nFig 2. Collective anomalies. | Image by [Anomaly detection for time series data: Part1](https://tech.clevertap.com/anomaly-detection-for-time-series-data-part-1/).\\n\\nAnomaly detection algorithms\\n----------------------------\\n\\n**Level shift anomaly detection**\\n\\nTo detect level shift anomalies, we used [ADTK](https://adtk.readthedocs.io/en/stable/index.html) python package for unsupervised anomaly detection in time series data. This `adtk.detector.LevelShiftAD` function detects anomalies by using two sliding windows to traverse the data, and calculates the difference between the statistical measures (median or mean) of the latter window and the former window. This difference is used to generate a new time series, `s1`. It then use `(Q1-cIQR, Q3+cIQR)` of `s1`to determine the normal range (`c` is a factor). If the difference falls outside this normal range, it is considered an anomaly point.\\n\\n![Image 5](https://miro.medium.com/v2/resize:fit:700/1*3FLb9pgMHFLJ4fk3sRUHFw.png)\\n\\nFig 3. The process of level shift anomaly detection algorithm. | Image by the author.\\n\\nThere are some critical parameters that we need to decide:\\n\\n_Window size_\\n\\nThe `window size` determined the number of data point in a time window. The larger the parameter is, the less sensitive the model is, which means it is less likely to be disturbed by sudden disturbances.\\n\\n_c_\\n\\nThe factor `c` is used to determine the bound of normal range based on historical interquartile range. When the original data fluctuates more, a larger value of parameter `c`is suitable as it provides a wider normal range. When the original data fluctuates less, a smaller value of parameter `c`is suitable to narrow down the normal range.\\n\\nPlease check the [document](https://adtk.readthedocs.io/en/stable/api/detectors.html) for other parameters.\\n\\n**The advantages of this method:**\\n\\n*   Robust and highly effective in detecting sudden changes in the baseline level\\n*   Easy to implement\\n*   The detection process can be customized by adjusting parameters to meet specific requirements.\\n\\n**The disadvantages of this method:**\\n\\n*   This method is primarily designed for detecting level shift anomalies and may not be as effective in identifying other types of anomalies. Additional detectors may be needed to complement its functionality\\n*   Sensitive to parameter tuning\\n*   Rely on the assumption of stationary data\\n\\n**Isolation forest**\\n\\nTo identify collective anomalies, we adopted `Isolation Forest`method. It is based on the principle that anomalies are rare and that there is a significant difference between anomalies and normal points. Isolation forest detect anomalies by measuring how far a data point is to the rest of the data through a tree-based algorithm and can run in a linear time complexity.\\n\\nSuppose we randomly partition a group of data points, where each partition divides the data points into two subspaces. This random partitioning process continues until each subspace contains only one data point. We can represent each partitioning step using a binary tree.\\n\\n![Image 6](https://miro.medium.com/v2/resize:fit:700/1*ZphiQ7ICKrTKBiEtilbMjA.png)\\n\\nFig 4. The example of how Isolation forest separated data points. | Image by the author.\\n\\nFrom Fig 4, we can observe that as the anomalous data points are far away from normal data points, they can be distinguished with fewer partitioning steps. On the other hand, normal data points, being densely clustered, require more partitioning steps to differentiate each data point. The number of partitioning steps corresponds to the tree depth or the path length from the root to the leaf in the binary tree. Therefore, if a data point has a shorter path length, it is likely to be an anomaly.\\n\\nThe whole algorithm starts from training `n` binary trees based on random forest method (Fig 5). Each data point is then passed through these `n`trees to obtain path lengths. The [original paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) then use a formula to calculate an average anomaly score ranging from 0 to 1 based on the obtained path lengths. Next, a threshold is defined to classify anomalies. Typically, a threshold of 0.5 is used, where an anomaly score < 0.5 indicates normal data, while an anomaly score > 0.5 indicates an anomaly.\\n\\n![Image 7](https://miro.medium.com/v2/resize:fit:700/1*bnn0BFjdXbXff6RCVhV7-A.png)\\n\\nFig 5. The process of training isolation forest with a dataset.| Image by the author.\\n\\n**The advantage of isolation forest:**\\n\\n*   **Robust**: Isolation forest is an ensemble method that combines the results of multiple isolation trees. The final anomaly score is calculated by averaging the scores from individual trees. This ensemble approach helps in reducing the effects of randomness and variations, making the algorithm more robust and reliable. Typically, the more trees there are, the more stable the algorithm becomes.\\n*   **Scalable**: Isolation Forest can be used on datasets with a massive amount of data as it uses a random partitioning strategy to create isolation trees, which makes it less sensitive to increasing data size compared to other traditional outlier detection algorithms.\\n*   It can handle **high-dimensional data**: Isolation Forest can handle high-dimensional datasets efficiently as it uses random feature selection at each split, reducing the impact of irrelevant or redundant features and focusing on the most informative ones.\\n*   **Insensitive to data distribution**: Isolation Forest does not assume any specific data distribution. It works well with data that may exhibit complex and nonlinear patterns. It is less affected by outliers, noise, or the presence of irrelevant features.\\n*   **Linear complexity**: Apart from the characteristics of random partitioning and random feature selection, isolation forest does not require calculating distance or density measures between data points like other anomaly detection methods, which can significantly improve speed and reduce computational overhead.\\n\\n**The disadvantage of isolation forest:**\\n\\n*   **Sensitivity to the number of anomalies**: Isolation forest\\xe2\\x80\\x99s performance can be influenced by the proportion of anomalies in the dataset. When the percentage of anomalies is high, the algorithm may struggle to effectively isolate them and distinguish them from normal data points. Tuning the contamination parameter (proportion of anomalies) becomes crucial in achieving accurate results.\\n*   **Ineffectiveness in detecting contextual anomalies**: Isolation Forest focuses on identifying data points that are different in terms of attribute values. However, it may not perform well in detecting contextual anomalies that are defined by their relationships with other data points.\\n*   **Lack of temporal awareness**: Isolation Forest treats each data point independently and does not explicitly consider the temporal ordering and dependencies in time series data. It may not effectively capture anomalies that rely on the sequential nature of the data or exhibit complex temporal patterns.\\n\\nImplementation example\\n----------------------\\n\\nThe following is an example of a real-world scenario. Some topics will be covered:\\n\\n*   What\\xe2\\x80\\x99s the problem we encountered? What\\xe2\\x80\\x99s the impact to our business?\\n*   The anomaly detection code. [github](https://github.com/YuRongTsao/anomaly-detection/tree/main)\\n*   How this method solved the problem.\\n\\n**Background**\\n\\nThat\\xe2\\x80\\x99s back to 2022/11/13. Assuming we are in a company that has not developed an anomaly detection system yet. We noticed a decrease in the number of users using a specific feature of our app. We wanted to determine whether it was a decrease in overall DAU or if only that specific feature was experiencing issues. To investigate, we retrieved the overall DAU data and check the trend.\\n\\n\\\\# Load data  \\nimport pandas as pd  \\nimport plotly.graph\\\\_objects as godef load\\\\_data(filename,x\\\\_column,y\\\\_column):  \\n    df = pd.read\\\\_csv(filename)  \\n    df.index = pd.to\\\\_datetime(df\\\\[x\\\\_column\\\\])  \\n    df.drop(columns=x\\\\_column,inplace=True)  \\n    df\\\\[y\\\\_column\\\\] = df\\\\[y\\\\_column\\\\].astype(float)  \\n    return df\\n\\ndef plot\\\\_base\\\\_line(df,y\\\\_column):  \\n    figure = go.Figure()  \\n    figure.add\\\\_trace(go.Scatter(name=y\\\\_column, x = df.index, y=df\\\\[y\\\\_column\\\\], marker=dict(color=\\'rgba(50,50,50,0.3)\\')))  \\n    figure.show()\\n\\nfilename = \\'dau.csv\\'  \\nx\\\\_column = \\'Date\\'  \\ny\\\\_column = \\'DAU\\'  \\ndf = load\\\\_data(filename,x\\\\_column,y\\\\_column)\\n\\nplot\\\\_base\\\\_line(df,y\\\\_column)\\n\\n![Image 8](https://miro.medium.com/v2/resize:fit:700/1*yuVr4GegtvsgLH-x5N-N4g.png)\\n\\nFig 6. DAU data from 2022\\xe2\\x80\\x9309\\xe2\\x80\\x9301 to 2022\\xe2\\x80\\x9311\\xe2\\x80\\x9313. | Image by the author.\\n\\n**What happened to the DAU?**\\n\\nWe discovered a significant decline on 2022/11/03. This coincided with the release of a new version of the app on 2022/11/02. We found a certain bug which may prevent users from updating the app, and the duration of the DAU drop was approximately 7 days (from 11/1 to 11/7), consistent with the phenomenon of app staged rollout. Then, we fixed the bugs on November 13, 2022, leading to a subsequent recovery in DAU.\\n\\nThe biggest issue in this scenario is that\\n\\n> We discovered the problem several days after it occurred (about 10 days!), which had already affected user experience and resulted in revenue loss.\\n\\nTherefore, we wish to develop an anomaly detection system that can detect anomalies and notify us as soon as the problem raised.\\n\\n**The Level shift anomaly detection module**\\n\\nWe implemented the level shift anomaly detection algorithm to detect changes in data levels.\\n\\nfrom adtk.data import validate\\\\_series  \\nfrom adtk.detector import LevelShiftAD  \\nimport warningswarnings.simplefilter(action=\"ignore\", category=UserWarning)\\n\\ndef level\\\\_shift\\\\_anomaly(series,config={\\'c\\':1,\\'side\\':\\'both\\',\\'window\\':3}):  \\n    s = validate\\\\_series(series)  \\n    model = LevelShiftAD(c=config\\\\[\\'c\\'\\\\], side=config\\\\[\\'side\\'\\\\],window = config\\\\[\\'window\\'\\\\])              \\n    anomalies = model.fit\\\\_detect(s)  \\n    return anomalies\\n\\ndef \\\\_join\\\\_df\\\\_with\\\\_anomaly(df, anomalies, anomaly\\\\_column):  \\n    anomalies = pd.DataFrame(anomalies)  \\n    anomalies.fillna(False, inplace=True)  \\n    anomalies.reset\\\\_index(drop=True)  \\n    df\\\\[anomaly\\\\_column\\\\]=anomalies.to\\\\_numpy()  \\n    return df\\n\\ndef plot\\\\_anomalies(df, y\\\\_column, config):  \\n    figure = go.Figure()  \\n    # plot baseline       \\n    figure.add\\\\_trace(go.Scatter(name=y\\\\_column, x = df.index, y=df\\\\[y\\\\_column\\\\], marker=dict(color=\\'rgba(50,50,50,0.3)\\')))\\n\\n# plot anomaly points       \\n    anomaly\\\\_df = df  \\n    anomaly\\\\_df = anomaly\\\\_df\\\\[anomaly\\\\_df\\\\[config\\\\[\\'anomaly\\\\_column\\'\\\\]\\\\]==True\\\\]\\n\\nfigure.add\\\\_trace(go.Scatter(name=config\\\\[\\'legend\\\\_name\\'\\\\], x = anomaly\\\\_df.index, y=anomaly\\\\_df\\\\[y\\\\_column\\\\],   \\n        mode=\\'markers\\',  \\n        marker=dict(color=config\\\\[\\'color\\'\\\\],size=10)))\\n\\nfigure.update\\\\_layout(  \\n            title= \\'DAU (simulated)\\',  \\n            xaxis\\\\_title=\\'date\\',  \\n            yaxis\\\\_title=\\'DAU\\',  \\n            legend\\\\_title=\"Anomaly Type\",  \\n        )\\n\\nfigure.show()\\n\\ndf = df\\\\[\\'2022-09-01\\':\\'2022-11-05\\'\\\\]  \\nconfig={  \\n    \\'anomaly\\\\_column\\':\\'levelshift\\\\_ad\\',  \\n    \\'legend\\\\_name\\': \\'levelshift anomaly\\',  \\n    \\'color\\':\\'rgba(249,123,34,0.8)\\'  \\n}\\n\\nanomalies = level\\\\_shift\\\\_anomaly(df)  \\ndf\\\\_anomalies = \\\\_join\\\\_df\\\\_with\\\\_anomaly(df,anomalies,config\\\\[\\'anomaly\\\\_column\\'\\\\])  \\nplot\\\\_anomalies(df\\\\_anomalies,y\\\\_column,config)\\n\\n![Image 9](https://miro.medium.com/v2/resize:fit:700/1*xFcUiFbv838VYkF0ipKEpw.png)\\n\\nFig 7. Level shift anomaly detection result. When it is 11/05, the system raised a level shift alert on 11/03. | Image by the author\\n\\nDuring the backtesting process, we observed that when the date reached 11/05, the system was already able to provide us with an alert on 11/03. However, when the date reached 11/09, using only the level shift method was insufficient in consistently providing local anomaly alerts. Therefore, we decided to incorporate the isolation forest algorithm to detect local anomalies.\\n\\n**The Isolation forest anomaly detection module**\\n\\nTo detect the local anomalies, we implemented the isolation forest anomaly detection algorithm.\\n\\nfrom sklearn.ensemble import IsolationForest  \\nimport warnings  \\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)def isolation\\\\_forest(df):  \\n    df\\\\_without\\\\_index = df.reset\\\\_index(drop=True)  \\n    model = IsolationForest(bootstrap=True,contamination=0.2, max\\\\_samples=0.2)  \\n    model.fit(df\\\\_without\\\\_index)  \\n    anomalies = pd.Series(model.predict(df\\\\_without\\\\_index)).apply(lambda x: True if (x == -1) else False)  \\n    return anomalies\\n\\nconfig={  \\n    \\'anomaly\\\\_column\\':\\'isolation\\\\_ad\\',  \\n    \\'legend\\\\_name\\': \\'collective anomaly\\',  \\n    \\'color\\':\\'rgba(255,217,61,0.8)\\'  \\n}\\n\\nanomalies = isolation\\\\_forest(df)  \\ndf\\\\_anomalies = \\\\_join\\\\_df\\\\_with\\\\_anomaly(df\\\\_anomalies,anomalies,config\\\\[\\'anomaly\\\\_column\\'\\\\])  \\nplot\\\\_anomalies(df\\\\_anomalies,y\\\\_column,config)\\n\\n![Image 10](https://miro.medium.com/v2/resize:fit:700/1*3P1vDCz0xqfnEUfpQ9uUQw.png)\\n\\nFig 8. Isolation forest result. The system provide alerts of collective anomalies when it\\xe2\\x80\\x99s on 11/12. | Image by the author\\n\\nFrom Fig 8, we found that the isolation forest method can consistently provide collective anomalies when the value is still abnormal.\\n\\n**Result**\\n\\nFinally, we present the combined results of both algorithms in a single graph. We used an orange dot to mark the level shift anomaly, a yellow for collective anomaly, and a red for both (Fig 9).\\n\\ndef plot(df, y\\\\_column, configs):figure = go.Figure()\\n\\n# plot baseline  \\n    figure.add\\\\_trace(go.Scatter(name=y\\\\_column, x = df.index, y=df\\\\[y\\\\_column\\\\], marker=dict(color=\\'rgba(50,50,50,0.3)\\')))\\n\\n# plot both levelshift and collective anomalies\\n\\nfor config in configs:  \\n        anomaly\\\\_df = df  \\n        for anomaly\\\\_type, status in config\\\\[\\'conditions\\'\\\\].items():  \\n            anomaly\\\\_df = anomaly\\\\_df\\\\[anomaly\\\\_df\\\\[anomaly\\\\_type\\\\]==status\\\\]\\n\\nfigure.add\\\\_trace(go.Scatter(name=config\\\\[\\'legend\\\\_name\\'\\\\], x = anomaly\\\\_df.index, y=anomaly\\\\_df\\\\[y\\\\_column\\\\],   \\n            mode=\\'markers\\',  \\n            marker=dict(color=config\\\\[\\'style\\'\\\\]\\\\[\\'color\\'\\\\],size=10)))\\n\\nfigure.update\\\\_layout(  \\n            title= \\'DAU (simulated)\\',  \\n            xaxis\\\\_title=\\'date\\',  \\n            yaxis\\\\_title=\\'DAU\\',  \\n            legend\\\\_title=\"Anomaly Type\",  \\n        )\\n\\nfigure.show()  \\n    figure.write\\\\_image(\"img/anomlay.png\")\\n\\nconfigs=\\\\[  \\n    {  \\n        \\'anomaly\\\\_column\\':\\'levelshift\\\\_ad\\',  \\n        \\'legend\\\\_name\\':\\'Level shift warning\\',  \\n        \\'conditions\\':{  \\n            \\'levelshift\\\\_ad\\':True,  \\n            \\'isolation\\\\_ad\\':False,  \\n        },  \\n        \\'style\\':{  \\n            \\'color\\': \\'rgba(249,123,34,0.8)\\',  \\n            \\'marker\\\\_size\\': 10  \\n        }  \\n    },  \\n    {  \\n        \\'anomaly\\\\_column\\':\\'isolation\\\\_ad\\',  \\n        \\'legend\\\\_name\\':\\'Collective warning\\',  \\n        \\'conditions\\':{  \\n            \\'levelshift\\\\_ad\\':False,  \\n            \\'isolation\\\\_ad\\':True,  \\n        },  \\n        \\'style\\':{  \\n            \\'color\\': \\'rgba(255,217,61,0.8)\\',  \\n            \\'marker\\\\_size\\': 10  \\n        }  \\n    },  \\n    {  \\n        \\'legend\\\\_name\\':\\'Overlap warning\\',  \\n        \\'conditions\\':{  \\n            \\'levelshift\\\\_ad\\':True,  \\n            \\'isolation\\\\_ad\\':True,  \\n        },  \\n        \\'style\\':{  \\n            \\'color\\': \\'rgba(223,46,56,0.8)\\',  \\n            \\'marker\\\\_size\\': 10  \\n        }  \\n    },  \\n\\\\]\\n\\nplot(df\\\\_anomalies,y\\\\_column,configs)\\n\\n![Image 11](https://miro.medium.com/v2/resize:fit:700/1*iLcxByA2y9Pdb3HC9ZgbrA.png)\\n\\nFig 9. The final result of all anomalies. | Image by the author.\\n\\nWe then conducted a backtesting experiment to test the feedback provided by the system from 2022/11/05 to 2022/11/17 (Fig 10). We found that on 11/05, the system detected a level shift anomaly occurred on 11/03. Compared to the actual occurrence of the problem (11/02), we were able to receive alerts within three days. This indicates that the detection was effective in providing early alerts, enabling us to initiate investigations sooner. Furthermore, as the date progressed to 11/13, the system consistently issued alerts for values below the normal range, indicating that the metric had not yet returned to normal.\\n\\n![Image 12](https://miro.medium.com/v2/resize:fit:700/1*GIKXCgnHXU0nHYOLzJoGaA.gif)\\n\\nFig 10. Backtesting from 11/05 to 11/17. | Image by the author.\\n\\nHooray! Now we can apply this anomaly detection system to all the other metrics so we can get early warning and prevent the revenue loss!\\n\\nNotes: you can find the full code on [GitHub](https://github.com/YuRongTsao/anomaly-detection/tree/main).\\n\\nSummary\\n-------\\n\\nIn this article, we discussed about:\\n\\n*   Why our business needs anomaly detection?\\n*   How to choose the anomaly detection algorithms and their pros and cons\\n*   The implementation details in python\\n*   A real-world example. Includes the encountered problem of the business scenario and an effective result of reducing the delay of alerts to within 3 days\\n',\n",
       " '_content_consumed': True,\n",
       " '_next': None,\n",
       " 'status_code': 200,\n",
       " 'headers': {'Date': 'Wed, 05 Jun 2024 15:11:30 GMT', 'Content-Type': 'text/plain; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-powered-by': 'Express', 'access-control-allow-origin': '*', 'x-cloud-trace-context': '55a147763c6bc7eddd262316d1ba1d10', 'CF-Cache-Status': 'DYNAMIC', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=H%2FLqL8oz2J7xp09Bkum6o1ibQqMC8TXpoR3I2lcoecwDcRSnz1QNjhfThNtIpskCI1uu3MIvZyPtKdejhsuUV4gm1R%2BcdSxzBh38xW71YxhVQH%2BPcaR8DBMwKQ%3D%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Server': 'cloudflare', 'CF-RAY': '88f11862dbad7fc7-MAA', 'Content-Encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'},\n",
       " 'raw': <urllib3.response.HTTPResponse at 0x1168a9700>,\n",
       " 'url': 'https://r.jina.ai/https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37',\n",
       " 'encoding': 'utf-8',\n",
       " 'history': [],\n",
       " 'reason': 'OK',\n",
       " 'cookies': <RequestsCookieJar[]>,\n",
       " 'elapsed': datetime.timedelta(seconds=3, microseconds=160576),\n",
       " 'request': <PreparedRequest [GET]>,\n",
       " 'connection': <requests.adapters.HTTPAdapter at 0x1168a9fd0>}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9WgXlceJubSNApQeXZPZcQw0gb1Lp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\'meta\\': {\\'status\\': \\'success\\', \\'message\\': \\'Data parsed successfully.\\'}, \\'data\\': {\\'article_insights\\': {\\'title\\': \\'How to perform anomaly detection in time series data with python? Methods, Code, Example!\\', \\'url\\': \\'https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37\\', \\'published_time\\': \\'2023-06-30T04:11:11.678Z\\', \\'key_topics\\': [\\'Why our business needs anomaly detection?\\', \\'Types of time series anomalies\\', \\'Anomaly detection algorithms\\', \\'Implementation example\\'], \\'summary_200\\': \"In this article, the importance of anomaly detection in time series data for businesses is discussed. The types of anomalies, including Level Shift Anomalies and Collective Anomalies, are explained. Two anomaly detection algorithms, Level Shift Anomaly Detection using ADTK and Isolation Forest, are detailed along with their advantages and disadvantages. An implementation example is provided, showcasing how to detect anomalies in a real-world scenario and the results obtained from using both algorithms.\", \\'code_snippets\\': [{\\'language\\': \\'python\\', \\'code\\': \\'import pandas as pd\\\\nimport plotly.graph_objects as go\\\\ndef load_data(filename, x_column, y_column):\\\\n    df = pd.read_csv(filename)\\\\n    df.index = pd.to_datetime(df[x_column])\\\\n    df.drop(columns=x_column, inplace=True)\\\\n    df[y_column] = df[y_column].astype(float)\\\\n    return df\\\\n\\\\n# More code snippets available in the article for anomaly detection in time series data.\\'}], \\'links\\': [\\'https://rapidminer.com/glossary/anomaly-detection/\\', \\'https://adtk.readthedocs.io/en/stable/index.html\\', \\'https://adtk.readthedocs.io/en/stable/userguide.html\\', \\'https://tech.clevertap.com/anomaly-detection-for-time-series-data-part-1/\\', \\'https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\\', \\'https://github.com/YuRongTsao/anomaly-detection/tree/main\\']}}}\\n', role='assistant', function_call=None, tool_calls=None))], created=1717577061, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=454, prompt_tokens=5942, total_tokens=6396))\n"
     ]
    }
   ],
   "source": [
    "response , client = query_system_expert(system_content=system_prompt_full,\n",
    "                                        user_input=str(content),max_tokens=1024)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message rcvd:\n",
      "{'id': 'chatcmpl-9WgXlceJubSNApQeXZPZcQw0gb1Lp', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\'meta\\': {\\'status\\': \\'success\\', \\'message\\': \\'Data parsed successfully.\\'}, \\'data\\': {\\'article_insights\\': {\\'title\\': \\'How to perform anomaly detection in time series data with python? Methods, Code, Example!\\', \\'url\\': \\'https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37\\', \\'published_time\\': \\'2023-06-30T04:11:11.678Z\\', \\'key_topics\\': [\\'Why our business needs anomaly detection?\\', \\'Types of time series anomalies\\', \\'Anomaly detection algorithms\\', \\'Implementation example\\'], \\'summary_200\\': \"In this article, the importance of anomaly detection in time series data for businesses is discussed. The types of anomalies, including Level Shift Anomalies and Collective Anomalies, are explained. Two anomaly detection algorithms, Level Shift Anomaly Detection using ADTK and Isolation Forest, are detailed along with their advantages and disadvantages. An implementation example is provided, showcasing how to detect anomalies in a real-world scenario and the results obtained from using both algorithms.\", \\'code_snippets\\': [{\\'language\\': \\'python\\', \\'code\\': \\'import pandas as pd\\\\nimport plotly.graph_objects as go\\\\ndef load_data(filename, x_column, y_column):\\\\n    df = pd.read_csv(filename)\\\\n    df.index = pd.to_datetime(df[x_column])\\\\n    df.drop(columns=x_column, inplace=True)\\\\n    df[y_column] = df[y_column].astype(float)\\\\n    return df\\\\n\\\\n# More code snippets available in the article for anomaly detection in time series data.\\'}], \\'links\\': [\\'https://rapidminer.com/glossary/anomaly-detection/\\', \\'https://adtk.readthedocs.io/en/stable/index.html\\', \\'https://adtk.readthedocs.io/en/stable/userguide.html\\', \\'https://tech.clevertap.com/anomaly-detection-for-time-series-data-part-1/\\', \\'https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\\', \\'https://github.com/YuRongTsao/anomaly-detection/tree/main\\']}}}\\n', role='assistant', function_call=None, tool_calls=None))], 'created': 1717577061, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=454, prompt_tokens=5942, total_tokens=6396)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\'meta\\': {\\'status\\': \\'success\\', \\'message\\': \\'Data parsed successfully.\\'}, \\'data\\': {\\'article_insights\\': {\\'title\\': \\'How to perform anomaly detection in time series data with python? Methods, Code, Example!\\', \\'url\\': \\'https://medium.com/@goldengoat/how-to-perform-anomaly-detection-in-time-series-data-with-python-methods-code-example-e83b9c951a37\\', \\'published_time\\': \\'2023-06-30T04:11:11.678Z\\', \\'key_topics\\': [\\'Why our business needs anomaly detection?\\', \\'Types of time series anomalies\\', \\'Anomaly detection algorithms\\', \\'Implementation example\\'], \\'summary_200\\': \"In this article, the importance of anomaly detection in time series data for businesses is discussed. The types of anomalies, including Level Shift Anomalies and Collective Anomalies, are explained. Two anomaly detection algorithms, Level Shift Anomaly Detection using ADTK and Isolation Forest, are detailed along with their advantages and disadvantages. An implementation example is provided, showcasing how to detect anomalies in a real-world scenario and the results obtained from using both algorithms.\", \\'code_snippets\\': [{\\'language\\': \\'python\\', \\'code\\': \\'import pandas as pd\\\\nimport plotly.graph_objects as go\\\\ndef load_data(filename, x_column, y_column):\\\\n    df = pd.read_csv(filename)\\\\n    df.index = pd.to_datetime(df[x_column])\\\\n    df.drop(columns=x_column, inplace=True)\\\\n    df[y_column] = df[y_column].astype(float)\\\\n    return df\\\\n\\\\n# More code snippets available in the article for anomaly detection in time series data.\\'}], \\'links\\': [\\'https://rapidminer.com/glossary/anomaly-detection/\\', \\'https://adtk.readthedocs.io/en/stable/index.html\\', \\'https://adtk.readthedocs.io/en/stable/userguide.html\\', \\'https://tech.clevertap.com/anomaly-detection-for-time-series-data-part-1/\\', \\'https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\\', \\'https://github.com/YuRongTsao/anomaly-detection/tree/main\\']}}}\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = extract_message_choice(response)\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'meta': {'status': 'success', 'message': 'Data parsed successfully.'}, 'data': {'article_insights': {'title': 'How to perform anomaly detection in time series data with python? Methods, Code, Example!', 'url': 'https://medium.com/@goldengoat/how-to-perform\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message rcvd:\n",
      "{'id': 'chatcmpl-9WgiZBv2hfwaESb7wpuB5d8nsOduF', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Let's expand on the key topics I mentioned earlier:\\n\\n1. Sustainability: Sustainability refers to the practice of using resources in a way that meets current needs without compromising the ability of future generations to meet their own needs. This includes reducing waste, conserving energy, promoting renewable resources, and protecting ecosystems. Businesses are increasingly adopting sustainable practices as consumers become more environmentally conscious and demand eco-friendly products and services.\\n\\n2. Diversity and Inclusion: Diversity and inclusion are essential for creating a more equitable and inclusive society. Diversity refers to the presence of different types of people in a group or organization, such as people of different races, genders, ages, and backgrounds. Inclusion, on the other hand, involves creating a culture where all individuals feel respected, valued, and included. Promoting diversity and inclusion in the workplace can lead to greater innovation, creativity, and productivity.\\n\\n3. Digital Transformation: Digital transformation refers to the integration of digital technologies into all aspects of a business, fundamentally changing how it operates and delivers value to customers. This includes adopting cloud computing, data analytics, artificial intelligence, and other digital tools to streamline processes, improve decision-making, and enhance customer experiences. Embracing digital transformation is crucial for businesses to stay competitive in today's rapidly evolving digital landscape.\\n\\nI hope this provides a more in-depth understanding of these key topics! Let me know if you have any other questions.\", role='assistant', function_call=None, tool_calls=None))], 'created': 1717577731, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=280, prompt_tokens=15, total_tokens=295)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! Let's expand on the key topics I mentioned earlier:\\n\\n1. Sustainability: Sustainability refers to the practice of using resources in a way that meets current needs without compromising the ability of future generations to meet their own needs. This includes reducing waste, conserving energy, promoting renewable resources, and protecting ecosystems. Businesses are increasingly adopting sustainable practices as consumers become more environmentally conscious and demand eco-friendly products and services.\\n\\n2. Diversity and Inclusion: Diversity and inclusion are essential for creating a more equitable and inclusive society. Diversity refers to the presence of different types of people in a group or organization, such as people of different races, genders, ages, and backgrounds. Inclusion, on the other hand, involves creating a culture where all individuals feel respected, valued, and included. Promoting diversity and inclusion in the workplace can lead to greater innovation, creativity, and productivity.\\n\\n3. Digital Transformation: Digital transformation refers to the integration of digital technologies into all aspects of a business, fundamentally changing how it operates and delivers value to customers. This includes adopting cloud computing, data analytics, artificial intelligence, and other digital tools to streamline processes, improve decision-making, and enhance customer experiences. Embracing digital transformation is crucial for businesses to stay competitive in today's rapidly evolving digital landscape.\\n\\nI hope this provides a more in-depth understanding of these key topics! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to ask follow up \n",
    "from constants import *\n",
    "user_input = \"expand on the key topics you just mentioned\"\n",
    "new_response = client.chat.completions.create(model=DEFAULT_MODEL,\n",
    "                                messages=[\n",
    "                                        {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\":user_input\n",
    "                                        }],\n",
    "                                temperature=DEFAULT_TEMPERATURE,\n",
    "                                max_tokens=1024,\n",
    "                                top_p=DEFAULT_TOP_P\n",
    "                            )\n",
    "fmsg = extract_message_choice(new_response)\n",
    "fmsg\n",
    "## client cant be treated like a state store. openai  treats each request independently. so to achieve the above req\n",
    "## store the history on our end, update the messages key in session for each expert. append the user/assistant response as they pile up\n",
    "## build mechanisms based on speciific use to optimise the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## done. takeaways ? \n",
    "## sub divide this agent to extract smaller things , \n",
    "    #  -> one prompt/agent for extract clean content & story meta data (title, time author etc)\n",
    "    # -> another to extract summary in 200 words, 100 words, 500 words, etc \n",
    "    # -> extract key topics (5-10), -\n",
    "    # -> extract link / images \n",
    "## keep api structure uniform with meta for each agent to pass messages related to the content\n",
    "## reader api is pretty good at parsing article like content -> would be interesting to know what percentage of web content is in form of article?\n",
    "    # what percentage of \"NEW\" content is articles ? \n",
    "    # who are the top 100 contributors to the web in terms of internet agencies -> what percentage of those are primarily in article form? \n",
    "    # ( remove platforms  that have ugc, focus on expert driven content--event if its internet expert ) \n",
    "## count tokens before sending the request to openapi (use tik token? )\n",
    "## do this in gscript to add buttons to google sheet/google docs. create agent in app scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last thing , read members only article \n",
    "members_medium_article\n",
    "reader = read_web_content(members_medium_article)\n",
    "members_content = reader.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 4, 'prompt_tokens': 572, 'total_tokens': 576}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'INCOMPLETE_ARTICLE'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify article using a chunk of agent functionality prompt \n",
    "response , client = query_system_expert(system_content=system_prompt_chunk,\n",
    "                                        user_input=str(members_content),max_tokens=64)\n",
    "msg = extract_message_choice(response)\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-9WhI8O2Iu0xztAGuSxjdnDWI4WiEJ', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='UNIDENTIFIED_CONTENT', role='assistant', function_call=None, tool_calls=None))], 'created': 1717579936, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=4, prompt_tokens=665, total_tokens=669)}\n",
      "{'completion_tokens': 4, 'prompt_tokens': 665, 'total_tokens': 669}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'UNIDENTIFIED_CONTENT'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more test links\n",
    "openai_li = \"https://platform.openai.com/docs/examples\"\n",
    "rnd = \"https://github.com/BLarzalere/LSTM-Autoencoder-for-Anomaly-Detection/tree/masters\"\n",
    "reader = read_web_content(rnd)\n",
    "x_content = reader.content\n",
    "response , client = query_system_expert(system_content=system_prompt_chunk,\n",
    "                                        user_input=str(x_content),max_tokens=64)\n",
    "print(response.__dict__)\n",
    "msg = extract_message_choice(response)\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_content': b\"Title: OpenAI Platform\\n\\nURL Source: https://platform.openai.com/docs/examples\\n\\nMarkdown Content:\\nIntroducing GPT-4o: our fastest and most affordable flagship model\\n\\nPrompt examples\\n---------------\\n\\nExplore what's possible with some example prompts\\n\\nGrammar correction\\n\\nConvert ungrammatical statements into standard English.\\n\\nSummarize for a 2nd grader\\n\\nSimplify text to a level appropriate for a second-grade student.\\n\\nParse unstructured data\\n\\nCreate tables from unstructured text.\\n\\nEmoji Translation\\n\\nTranslate regular text into emoji text.\\n\\nCalculate time complexity\\n\\nFind the time complexity of a function.\\n\\nExplain code\\n\\nExplain a complicated piece of code.\\n\\nKeywords\\n\\nExtract keywords from a block of text.\\n\\nProduct name generator\\n\\nGenerate product names from a description and seed words.\\n\\nPython bug fixer\\n\\nFind and fix bugs in source code.\\n\\nSpreadsheet creator\\n\\nCreate spreadsheets of various kinds of data.\\n\\nTweet classifier\\n\\nDetect sentiment in a tweet.\\n\\nAirport code extractor\\n\\nExtract airport codes from text.\\n\\nMood to color\\n\\nTurn a text description into a color.\\n\\nVR fitness idea generator\\n\\nGenerate ideas for fitness promoting virtual reality games.\\n\\nMarv the sarcastic chat bot\\n\\nMarv is a factual chatbot that is also sarcastic.\\n\\nTurn by turn directions\\n\\nConvert natural language to turn-by-turn directions.\\n\\nInterview questions\\n\\nCreate interview questions.\\n\\nFunction from specification\\n\\nCreate a Python function from a specification.\\n\\nImprove code efficiency\\n\\nProvide ideas for efficiency improvements to Python code.\\n\\nSingle page website creator\\n\\nCreate a single page website.\\n\\nRap battle writer\\n\\nGenerate a rap battle between two characters.\\n\\nMemo writer\\n\\nGenerate a company memo based on provided points.\\n\\nEmoji chatbot\\n\\nGenerate conversational replies using emojis only.\\n\\nTranslation\\n\\nTranslate natural language text.\\n\\nSocratic tutor\\n\\nGenerate responses as a Socratic tutor.\\n\\nNatural language to SQL\\n\\nConvert natural language into SQL queries.\\n\\nMeeting notes summarizer\\n\\nSummarize meeting notes including overall discussion, action items, and future topics.\\n\\nReview classifier\\n\\nClassify user reviews based on a set of tags.\\n\\nPro and con discusser\\n\\nAnalyze the pros and cons of a given topic.\\n\\nLesson plan writer\\n\\nGenerate a lesson plan for a specific topic.\\n\",\n",
       " '_content_consumed': True,\n",
       " '_next': None,\n",
       " 'status_code': 200,\n",
       " 'headers': {'Date': 'Wed, 05 Jun 2024 09:21:11 GMT', 'Content-Type': 'text/plain; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-powered-by': 'Express', 'access-control-allow-origin': '*', 'x-cloud-trace-context': '0b4c4c42597231dea84786bb5e588afc', 'CF-Cache-Status': 'DYNAMIC', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=TU0XTiMdis2ChOUiD75Zh9dA4yy61s8SLexRRcI%2Fz8Eej2z0ZSu61K1EBm91iHQwqHibHsSqYr8rRb2IUjgaq3oZilC7WoMZaxNutqm3YNQq%2F4xKIFVZoQvUVrU%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Server': 'cloudflare', 'CF-RAY': '88ef173c48e87f73-MAA', 'Content-Encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'},\n",
       " 'raw': <urllib3.response.HTTPResponse at 0x11680c640>,\n",
       " 'url': 'https://r.jina.ai/https://platform.openai.com/docs/examples',\n",
       " 'encoding': 'utf-8',\n",
       " 'history': [],\n",
       " 'reason': 'OK',\n",
       " 'cookies': <RequestsCookieJar[]>,\n",
       " 'elapsed': datetime.timedelta(seconds=3, microseconds=88874),\n",
       " 'request': <PreparedRequest [GET]>,\n",
       " 'connection': <requests.adapters.HTTPAdapter at 0x11680cb50>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
