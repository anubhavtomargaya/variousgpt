{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this notebook aims to prepare EARNINGS CALL files for transcripton. that includes\n",
    "## - pre process to correct file formats/encoding \n",
    "## - take metadata about the audio\n",
    "## - submit a job for transcribing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read audio file (full) -> upload from user or LINK \n",
    "# input meta for the file \n",
    "# transcribe \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3 -> for dev purpose 'data' folder is s3 \n",
    "# pull audio -> create meta -> start processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta \n",
    "- filename \n",
    "- type \n",
    "- format \n",
    "- url \n",
    "- length of audio \n",
    "- org name \n",
    "- about org/convo [optional]\n",
    "- number of speakers [opt]\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note \n",
    "# transcript will be saved in the database as 'raw'.  post verification by user it is stored as final trx.\n",
    "# embedding happens post user validation \n",
    "# embedding strategy is default - tbd \n",
    "## default - full file, chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List \n",
    "from enums import tsFormats\n",
    "class RecordingFormats(Enum):\n",
    "    MP3 = 'mp3'\n",
    "    WAV = 'wav'\n",
    "    M4A = 'm4a'\n",
    "\n",
    "class RecordingTypes(Enum):\n",
    "   CONCALL = 'concall'\n",
    "   INTERVIEW = 'interview'\n",
    "   \n",
    "\n",
    "class AudioFileMeta:\n",
    "    def __init__(self, \n",
    "                 file_name:str,  #pk\n",
    "                 input_file_format:RecordingFormats,\n",
    "                 recording_type:RecordingTypes,\n",
    "                 audio_length:float=None,\n",
    "                 ) -> None:\n",
    "        \n",
    "        pass\n",
    "\n",
    "class TranscriptMetadata:\n",
    "    def __init__(self, \n",
    "                 audio_file_name:str,  #pk\n",
    "                 transcript_format:tsFormats,\n",
    "                 num_chunks:int=None,\n",
    "                 org_name:str=None, \n",
    "                 audio_base_prompt:str=None,\n",
    "                 speakers:dict=None\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.audio_file_name = audio_file_name\n",
    "        self.transcript_format = transcript_format\n",
    "        self.num_chunks = num_chunks\n",
    "        self.org_name = org_name\n",
    "        self.audio_base_prompt = audio_base_prompt\n",
    "        self.speakers = speakers\n",
    "        \n",
    "class TranscriptChunk:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 file_name,\n",
    "                 text,\n",
    "                 embedding=None) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_openai_client\n",
    "from utils_transcribe import transcribe_audio_in_format\n",
    "from utils_dir import save_transcript_text_json\n",
    "from utils_audio import chop_audio, convert_audio_to_ogg, open_audio_as_segment\n",
    "from dirs import *\n",
    "raw_dir = DATA_DIR\n",
    "ogg_dir = PROCESSED_DIR\n",
    "audio_file = 'earning_call_morepen.mp3'\n",
    "audio_type = RecordingTypes.CONCALL\n",
    "audio_format = RecordingFormats.MP3\n",
    "\n",
    "class AudioTranscript:\n",
    "    def __init__(self,\n",
    "                 text,\n",
    "                 meta:dict) -> None:\n",
    "        self.text = text \n",
    "        self.meta = meta \n",
    "\n",
    "# the following is one atomic chain \n",
    "def read_audio_file(file_name:str):\n",
    "    ogg_file = convert_audio_to_ogg(file_name=file_name)\n",
    "    if not ogg_file:\n",
    "        raise Exception(\"Unable to convert audio to OGG\")\n",
    "   \n",
    "    audio = open_audio_as_segment(ogg_file)\n",
    "    total_duration_milliseconds = len(audio)\n",
    "    total_duration_seconds = total_duration_milliseconds / 1000\n",
    "    meta = AudioFileMeta(file_name=file_name,\n",
    "                         input_file_format=RecordingFormats.MP3,\n",
    "                         recording_type=RecordingTypes.CONCALL,\n",
    "                         audio_length=total_duration_seconds)\n",
    "    \n",
    "\n",
    "    return meta, audio\n",
    "\n",
    "def insert_audio_meta(meta:AudioFileMeta):\n",
    "    pass\n",
    "\n",
    "def transcribe_audio_in_chunks(audio_path:Path,\n",
    "                            base_prompt:str,\n",
    "                            output_dir=TS_DIR): # save text of each chunk (audio chunk)\n",
    "    \n",
    "    \"\"\"Finish transcription, concat transcripts and save in a AudioTranscript object to a json.\n",
    "    ensure ogg file in full path is entered, base prompt to catch domain specific words.\n",
    "        chop file and save chunks in chop dir. Transcribe each chunk sequentially reading from disk. \n",
    "        \n",
    "    \"\"\"\n",
    "    meta = TranscriptMetadata(audio_file_name=audio_path.stem,\n",
    "                              transcript_format=tsFormats.JSON)\n",
    "    chunk_files = chop_audio(audio_path)\n",
    "    meta.num_chunks = len(chunk_files)\n",
    "    # start transcription of chunks, \n",
    "    client = get_openai_client()\n",
    "    print(\"starting transcriptioni, got client\")\n",
    "    prompt = base_prompt\n",
    "    output_prefix = 'conc_'\n",
    "    id = 0\n",
    "    transcript_final = AudioTranscript(text='',meta={'total_chunks':len(chunk_files),\n",
    "                                                     'count':0 \n",
    "                                                     }\n",
    "                                            )\n",
    "\n",
    "    print(\"chunk files:\", type(chunk_files), chunk_files)\n",
    "    for chunk_file in chunk_files:\n",
    "\n",
    "        file_path = Path(CHOP_DIR,chunk_file)\n",
    "        print(\"file path of chunk :\", file_path.__str__())\n",
    "        \n",
    "        trx = transcribe_audio_in_format(client=client,\n",
    "                                        audio_file_path= file_path ,\n",
    "                                        prompt=prompt)\n",
    "        prompt = trx.text #update prompt for next chunk \n",
    "        transcript_final.text += f'{trx.text} '\n",
    "        transcript_final.meta['count'] += 1\n",
    "        \n",
    "\n",
    "    out_file_name =  f'{output_prefix}{audio_path.stem}'\n",
    "\n",
    "    if save_transcript_text_json(transcript_final,\n",
    "                         out_file_name,\n",
    "                         dir=output_dir):\n",
    "\n",
    "        print(id, \"file processsed as ts: \", file_path.stem)\n",
    "        return f'{out_file_name}.json'\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "## prepare a database to store the transcripts after processing chopped files \n",
    "## preferably faiss or mongo db ( embeddings will be stored there as well )\n",
    "\n",
    "def insert_raw_transcript(chunk_meta):\n",
    "    pass \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_audio_file(file_name=audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ogg_file = 'earning_call_morepen.ogg'\n",
    "# audio = open_audio_as_segment(ogg_file)\n",
    "# audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_morepen = \"Embarking on a transformative journey over four decades, Morepen has consistently strived to embody an organization marked by discernible, impactful differences. Every action we undertake is imbued with a profound commitment to enrich the lives we touch. Nestled amidst the serene surroundings of Baddi, our cutting-edge manufacturing facility boasts a scientifically integrated complex housing 10 plants, each tailored to produce specific product lines.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ogg_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearning_call_morepen.ogg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(PROCESSED_DIR,ogg_file)\n\u001b[1;32m      4\u001b[0m output_file \u001b[38;5;241m=\u001b[39m transcribe_audio_in_chunks(base_prompt\u001b[38;5;241m=\u001b[39mabout_morepen,audio_path\u001b[38;5;241m=\u001b[39minput_path,)\n\u001b[1;32m      5\u001b[0m output_file\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "ogg_file = 'earning_call_morepen.ogg'\n",
    "input_path = Path(PROCESSED_DIR,ogg_file)\n",
    "\n",
    "output_file = transcribe_audio_in_chunks(base_prompt=about_morepen,audio_path=input_path,)\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## massage raw transcripts \n",
    "### raw transcripts are just blobs of text they can be enriched by \n",
    "# - giving to user for corrections \n",
    "# - giving to gpt for summarisation, diarization, formatting, topic extractions etc \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
